{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intent LLM Playground\n",
        "\n",
        "Use this notebook to exercise the intents classifier + slot filler end to end. Configure your Ollama endpoint, type some artifact text, and capture the raw responses before wiring them into the worker.\n",
        "\n",
        "> Tip: keep Ollama running (`ollama serve`) and ensure the requested model is already pulled. You can also swap in a mock responder if you want deterministic outputs (see the optional section below).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/chrispatten/workspace/haven\n",
            "Using Ollama endpoint: http://localhost:11434\n",
            "Model: llama3.2\n",
            "Taxonomy: /Users/chrispatten/workspace/haven/services/worker_service/taxonomies/taxonomy_v1.0.0.yaml\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Optional\n",
        "from uuid import uuid4\n",
        "\n",
        "import httpx\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Ensure the Haven project root (and src/) are importable before using modules\n",
        "# ---------------------------------------------------------------------------\n",
        "def resolve_project_root() -> Path:\n",
        "    env_root = Path(os.getenv(\"HAVEN_PROJECT_ROOT\", \"\")).expanduser()\n",
        "    if env_root and (env_root / \"src\" / \"haven\").exists():\n",
        "        return env_root\n",
        "\n",
        "    cwd = Path.cwd().resolve()\n",
        "    if (cwd / \"src\" / \"haven\").exists():\n",
        "        return cwd\n",
        "\n",
        "    if (cwd.parent / \"src\" / \"haven\").exists():\n",
        "        return cwd.parent\n",
        "\n",
        "    raise RuntimeError(\n",
        "        \"Unable to locate Haven project root. Set HAVEN_PROJECT_ROOT or launch the notebook from the repo root.\"\n",
        "    )\n",
        "\n",
        "\n",
        "PROJECT_ROOT = resolve_project_root()\n",
        "SRC_PATH = PROJECT_ROOT / \"src\"\n",
        "for candidate in (PROJECT_ROOT, SRC_PATH):\n",
        "    path_str = str(candidate)\n",
        "    if path_str not in sys.path:\n",
        "        sys.path.insert(0, path_str)\n",
        "\n",
        "from haven.intents.classifier.classifier import ClassifierSettings, classify_artifact\n",
        "from haven.intents.classifier.taxonomy import IntentTaxonomy, load_taxonomy\n",
        "from haven.intents.models import ClassificationResult\n",
        "from haven.intents.slots import SlotFiller, SlotFillerResult, SlotFillerSettings\n",
        "from shared.logging import setup_logging\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Configure runtime knobs here\n",
        "# ---------------------------------------------------------------------------\n",
        "OLLAMA_BASE_URL = 'http://localhost:11434' # os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "INTENT_MODEL = os.getenv(\"INTENT_MODEL\", \"llama3.2\")\n",
        "DEFAULT_TAXONOMY_PATH = PROJECT_ROOT / \"services/worker_service/taxonomies/taxonomy_v1.0.0.yaml\"\n",
        "\n",
        "# Optional: point to an alternate taxonomy payload if you're testing new definitions\n",
        "CUSTOM_TAXONOMY_PATH = os.getenv(\"INTENT_TAXONOMY_PATH\")\n",
        "\n",
        "setup_logging()\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Using Ollama endpoint: {OLLAMA_BASE_URL}\")\n",
        "print(f\"Model: {INTENT_MODEL}\")\n",
        "print(f\"Taxonomy: {CUSTOM_TAXONOMY_PATH or DEFAULT_TAXONOMY_PATH}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_intent_taxonomy(path: Optional[Path] = None) -> IntentTaxonomy:\n",
        "    resolved = Path(path or CUSTOM_TAXONOMY_PATH or DEFAULT_TAXONOMY_PATH).expanduser()\n",
        "    if not resolved.exists():\n",
        "        raise FileNotFoundError(f\"Taxonomy file not found: {resolved}\")\n",
        "    return load_taxonomy(resolved)\n",
        "\n",
        "\n",
        "def build_slot_filler() -> SlotFiller:\n",
        "    return SlotFiller(\n",
        "        SlotFillerSettings(\n",
        "            ollama_base_url=OLLAMA_BASE_URL,\n",
        "            slot_model=INTENT_MODEL,\n",
        "            request_timeout=30.0,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def run_intent_pipeline(\n",
        "    *,\n",
        "    text: str,\n",
        "    entities: Dict[str, Any],\n",
        "    source_type: str = \"imessage\",\n",
        "    artifact_id: Optional[str] = None,\n",
        "    client: Optional[httpx.Client] = None,\n",
        "    slot_filler: Optional[SlotFiller] = None,\n",
        "    taxonomy: Optional[IntentTaxonomy] = None,\n",
        ") -> tuple[ClassificationResult, SlotFillerResult]:\n",
        "    taxonomy = taxonomy or load_intent_taxonomy()\n",
        "    artifact_id = artifact_id or str(uuid4())\n",
        "    owns_client = client is None\n",
        "\n",
        "    if client is None:\n",
        "        client = httpx.Client(base_url=OLLAMA_BASE_URL, timeout=30.0)\n",
        "    if slot_filler is None:\n",
        "        slot_filler = build_slot_filler()\n",
        "\n",
        "    classifier_settings = ClassifierSettings(\n",
        "        base_url=OLLAMA_BASE_URL,\n",
        "        model=INTENT_MODEL,\n",
        "        timeout=30.0,\n",
        "        min_confidence=0.35,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        classification = classify_artifact(\n",
        "            text=text,\n",
        "            taxonomy=taxonomy,\n",
        "            entities=entities,\n",
        "            settings=classifier_settings,\n",
        "            client=client,\n",
        "        )\n",
        "        slot_result = slot_filler.fill_slots(\n",
        "            job_text=text,\n",
        "            classification=classification,\n",
        "            taxonomy=taxonomy,\n",
        "            entity_payload=entities,\n",
        "            artifact_id=artifact_id,\n",
        "            source_type=source_type,\n",
        "        )\n",
        "    finally:\n",
        "        if owns_client:\n",
        "            client.close()\n",
        "\n",
        "    return classification, slot_result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample artifact + entities\n",
        "\n",
        "Adjust the text/entities below to mirror the document you want to test. Including `channel_context.from` / `channel_context.to` helps the model resolve pronouns for messaging/email content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hey Chris â€” can you remember to pick up eggs for me before tomorrow night? \n",
            "I'll be tied up with meetings until late Wednesday.\n",
            "{\n",
            "  \"people\": [\n",
            "    {\n",
            "      \"normalizedValue\": \"Chris\",\n",
            "      \"identifier\": \"imessage:+15551234567\",\n",
            "      \"role\": \"recipient\"\n",
            "    }\n",
            "  ],\n",
            "  \"dates\": [\n",
            "    {\n",
            "      \"normalizedValue\": \"2025-11-12T21:00:00-05:00\",\n",
            "      \"entity\": {\n",
            "        \"text\": \"tomorrow night\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"channel_context\": {\n",
            "    \"from\": {\n",
            "      \"display_name\": \"Alex\",\n",
            "      \"identifier\": \"imessage:+15550987654\"\n",
            "    },\n",
            "    \"to\": [\n",
            "      {\n",
            "        \"display_name\": \"Chris\",\n",
            "        \"identifier\": \"imessage:+15551234567\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"\"\"\n",
        "Hey Chris â€” can you remember to pick up eggs for me before tomorrow night? \n",
        "I'll be tied up with meetings until late Wednesday.\n",
        "\"\"\".strip()\n",
        "\n",
        "sample_entities = {\n",
        "    \"people\": [\n",
        "        {\n",
        "            \"normalizedValue\": \"Chris\",\n",
        "            \"identifier\": \"imessage:+15551234567\",\n",
        "            \"role\": \"recipient\",\n",
        "        }\n",
        "    ],\n",
        "    \"dates\": [\n",
        "        {\n",
        "            \"normalizedValue\": \"2025-11-12T21:00:00-05:00\",\n",
        "            \"entity\": {\"text\": \"tomorrow night\"},\n",
        "        }\n",
        "    ],\n",
        "    \"channel_context\": {\n",
        "        \"from\": {\n",
        "            \"display_name\": \"Alex\",\n",
        "            \"identifier\": \"imessage:+15550987654\",\n",
        "        },\n",
        "        \"to\": [\n",
        "            {\n",
        "                \"display_name\": \"Chris\",\n",
        "                \"identifier\": \"imessage:+15551234567\",\n",
        "            }\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "print(sample_text)\n",
        "print(json.dumps(sample_entities, indent=2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "================================================================================\n",
            "CLASSIFIER OUTPUT\n",
            "================================================================================\n",
            "\n",
            "âœ“ Intent: reminder.create\n",
            "  Confidence: 75.00% (base: 75.00%, prior: 1.00x)\n",
            "  Reasons:\n",
            "    â€¢ 'remember to' suggests reminder.create\n",
            "    â€¢ The text contains a specific request for a follow-up action ('pick up eggs')\n",
            "\n",
            "================================================================================\n",
            "SLOT FILLING OUTPUT\n",
            "================================================================================\n",
            "\n",
            "ðŸ“‹ Intent: reminder.create (75.00%)\n",
            "  âœ“ Resolved Slots:\n",
            "    â€¢ source_ref (from default): d1178bdb-55a5-402b-82ba-5dc6607d9c20\n",
            "    â€¢ remind_at (from entity): 2025-11-12T21:00:00-05:00\n",
            "    â€¢ person (from entity): {\"name\": \"Chris\", \"identifier\": \"imessage:+15551234567\", \"role\": \"recipient\"}\n",
            "    â€¢ what (from llm): pick up eggs\n",
            "  â„¹ Notes:\n",
            "    â€¢ Although the conversation also mentions a task (picking up eggs), the 'reminder.create' intent is more strongly supported by the language and context, as it explicitly requests a follow-up action.\n",
            "\n",
            "================================================================================\n",
            "SUMMARY\n",
            "================================================================================\n",
            "Text analyzed: 'Hey Chris â€” can you remember to pick up eggs for me before t...'\n",
            "Source type: imessage\n",
            "Channel context: from=Alex, to=['Chris']\n",
            "Intents detected: 1\n",
            "Top intent: reminder.create\n",
            "Resolved slots: 4 / 4\n"
          ]
        }
      ],
      "source": [
        "classification, slot_result = run_intent_pipeline(\n",
        "    text=sample_text,\n",
        "    entities=sample_entities,\n",
        "    source_type=\"imessage\",\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CLASSIFIER OUTPUT\")\n",
        "print(\"=\" * 80)\n",
        "for intent in classification.intents:\n",
        "    print(f\"\\nâœ“ Intent: {intent.intent_name}\")\n",
        "    print(f\"  Confidence: {intent.confidence:.2%} (base: {intent.base_confidence:.2%}, prior: {intent.prior_applied:.2f}x)\")\n",
        "    if intent.reasons:\n",
        "        print(f\"  Reasons:\")\n",
        "        for reason in intent.reasons:\n",
        "            print(f\"    â€¢ {reason}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SLOT FILLING OUTPUT\")\n",
        "print(\"=\" * 80)\n",
        "for assignment in slot_result.assignments:\n",
        "    print(f\"\\nðŸ“‹ Intent: {assignment.intent_name} ({assignment.confidence:.2%})\")\n",
        "    \n",
        "    if assignment.slots:\n",
        "        print(f\"  âœ“ Resolved Slots:\")\n",
        "        for slot_name, value in assignment.slots.items():\n",
        "            source = assignment.slot_sources.get(slot_name, \"?\")\n",
        "            if isinstance(value, (dict, list)):\n",
        "                print(f\"    â€¢ {slot_name} (from {source}): {json.dumps(value)}\")\n",
        "            else:\n",
        "                print(f\"    â€¢ {slot_name} (from {source}): {value}\")\n",
        "    \n",
        "    if assignment.missing_slots:\n",
        "        print(f\"  âš  Missing Required Slots: {', '.join(assignment.missing_slots)}\")\n",
        "    \n",
        "    if assignment.notes:\n",
        "        print(f\"  â„¹ Notes:\")\n",
        "        for note in assignment.notes:\n",
        "            print(f\"    â€¢ {note}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Text analyzed: '{sample_text[:60]}...'\")\n",
        "print(f\"Source type: imessage\")\n",
        "print(f\"Channel context: from={sample_entities['channel_context']['from']['display_name']}, to={[p['display_name'] for p in sample_entities['channel_context']['to']]}\")\n",
        "print(f\"Intents detected: {len(classification.intents)}\")\n",
        "print(f\"Top intent: {classification.intents[0].intent_name if classification.intents else 'none'}\")\n",
        "if slot_result.assignments:\n",
        "    top_assignment = slot_result.assignments[0]\n",
        "    print(f\"Resolved slots: {len(top_assignment.slots)} / {len(top_assignment.slots) + len(top_assignment.missing_slots)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OLLAMA_BASE_URL: 'http://localhost:11434'\n",
            "INTENT_MODEL: 'llama3.2'\n",
            "\n",
            "=== RAW OLLAMA RESPONSE ===\n",
            "HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "{\n",
            "  \"intents\": [\n",
            "    {\n",
            "      \"name\": \"reminder.create\",\n",
            "      \"base_confidence\": 0.87,\n",
            "      \"reasons\": [\"Hey Chris â€” can you remember to pick up eggs for me before tomorrow night?\"]\n",
            "    }\n",
            "  ],\n",
            "  \"notes\": [\"The intent 'reminder.create' is supported because of the request to 'remember to' pick up eggs. The temporal context provided also suggests a reminder is being asked for.\"]\n",
            "}\n",
            "\n",
            "=== PARSED JSON ===\n",
            "{\n",
            "  \"intents\": [\n",
            "    {\n",
            "      \"name\": \"reminder.create\",\n",
            "      \"base_confidence\": 0.87,\n",
            "      \"reasons\": [\n",
            "        \"Hey Chris \\u2014 can you remember to pick up eggs for me before tomorrow night?\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"notes\": [\n",
            "    \"The intent 'reminder.create' is supported because of the request to 'remember to' pick up eggs. The temporal context provided also suggests a reminder is being asked for.\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "=== INTENTS ARRAY ===\n",
            "[\n",
            "  {\n",
            "    \"name\": \"reminder.create\",\n",
            "    \"base_confidence\": 0.87,\n",
            "    \"reasons\": [\n",
            "      \"Hey Chris \\u2014 can you remember to pick up eggs for me before tomorrow night?\"\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print(f\"OLLAMA_BASE_URL: {repr(OLLAMA_BASE_URL)}\")\n",
        "print(f\"INTENT_MODEL: {repr(INTENT_MODEL)}\")\n",
        "\n",
        "# Capture raw Ollama response before validation\n",
        "from haven.intents.classifier.classifier import _build_prompt, _invoke_ollama, _parse_response, ClassifierSettings as CS\n",
        "\n",
        "prompt = _build_prompt(\n",
        "    text=sample_text,\n",
        "    taxonomy=load_intent_taxonomy(),\n",
        "    entities=sample_entities,\n",
        "    min_confidence=0.35,\n",
        ")\n",
        "settings = CS(base_url=OLLAMA_BASE_URL, model=INTENT_MODEL, timeout=30.0, min_confidence=0.35)\n",
        "\n",
        "print(\"\\n=== RAW OLLAMA RESPONSE ===\")\n",
        "raw_text = _invoke_ollama(\n",
        "    payload={\"model\": INTENT_MODEL, \"prompt\": prompt, \"format\": \"json\", \"stream\": False},\n",
        "    settings=settings,\n",
        "    client=None,\n",
        ")\n",
        "print(raw_text)\n",
        "\n",
        "print(\"\\n=== PARSED JSON ===\")\n",
        "parsed = _parse_response(raw_text)\n",
        "print(json.dumps(parsed, indent=2))\n",
        "\n",
        "print(\"\\n=== INTENTS ARRAY ===\")\n",
        "print(json.dumps(parsed.get(\"intents\", []), indent=2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "Classifier intents:\n",
            "- reminder.create: confidence=0.75 (base=0.75)\n",
            "    reason: Hey Chris â€” can you remember to pick up eggs for me before tomorrow night?\n",
            "    reason: I'll be tied up with meetings until late Wednesday.\n",
            "\n",
            "Slot assignments:\n",
            "\n",
            "Intent: reminder.create (confidence=0.75)\n",
            "Resolved slots:\n",
            "  - source_ref: 24ae8f8c-b8cb-415f-adf9-78cbfcd5824d\n",
            "  - remind_at: 2025-11-12T21:00:00-05:00\n",
            "  - person: {'name': 'Chris', 'identifier': 'imessage:+15551234567', 'role': 'recipient'}\n",
            "  - what: pick up eggs\n",
            "Sources: {'source_ref': 'default', 'remind_at': 'entity', 'person': 'entity', 'what': 'llm'}\n",
            "Notes:\n",
            "  â€¢ Entity 'people' contains information about the person involved (Chris), and channel context provides display names and identifiers for both parties.\n"
          ]
        }
      ],
      "source": [
        "classification, slot_result = run_intent_pipeline(\n",
        "    text=sample_text,\n",
        "    entities=sample_entities,\n",
        "    source_type=\"imessage\",\n",
        ")\n",
        "\n",
        "print(\"Classifier intents:\")\n",
        "for intent in classification.intents:\n",
        "    print(f\"- {intent.intent_name}: confidence={intent.confidence:.2f} (base={intent.base_confidence:.2f})\")\n",
        "    if intent.reasons:\n",
        "        for reason in intent.reasons:\n",
        "            print(f\"    reason: {reason}\")\n",
        "\n",
        "print(\"\\nSlot assignments:\")\n",
        "for assignment in slot_result.assignments:\n",
        "    print(f\"\\nIntent: {assignment.intent_name} (confidence={assignment.confidence:.2f})\")\n",
        "    print(\"Resolved slots:\")\n",
        "    for name, value in assignment.slots.items():\n",
        "        print(f\"  - {name}: {value}\")\n",
        "    if assignment.missing_slots:\n",
        "        print(\"Missing required:\", assignment.missing_slots)\n",
        "    if assignment.slot_sources:\n",
        "        print(\"Sources:\", assignment.slot_sources)\n",
        "    if assignment.notes:\n",
        "        print(\"Notes:\")\n",
        "        for note in assignment.notes:\n",
        "            print(f\"  â€¢ {note}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: offline stub\n",
        "\n",
        "If you want predictable responses without calling Ollama, you can monkeypatch `httpx.Client.post` to return canned JSON. The snippet below shows one way to do it; uncomment and adjust as needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example stub for deterministic testing.\n",
        "#\n",
        "# from contextlib import contextmanager\n",
        "#\n",
        "# @contextmanager\n",
        "# def canned_llm_response(payload: Dict[str, Any]):\n",
        "#     original_post = httpx.Client.post\n",
        "#\n",
        "#     def fake_post(self, url, json=None, *args, **kwargs):\n",
        "#         # Only intercept Ollama generation calls\n",
        "#         if url == \"/api/generate\":\n",
        "#             return httpx.Response(\n",
        "#                 status_code=200,\n",
        "#                 json={\"response\": json.dumps(payload)},\n",
        "#             )\n",
        "#         return original_post(self, url, json=json, *args, **kwargs)\n",
        "#\n",
        "#     httpx.Client.post = fake_post\n",
        "#     try:\n",
        "#         yield\n",
        "#     finally:\n",
        "#         httpx.Client.post = original_post\n",
        "#\n",
        "# canned_response = {\n",
        "#     \"intents\": [\n",
        "#        {\n",
        "#            \"name\": \"reminder.create\",\n",
        "#            \"base_confidence\": 0.82,\n",
        "#            \"confidence\": 0.86,\n",
        "#            \"reasons\": [\n",
        "#                \"Follow-up request detected\",\n",
        "#                \"Contains explicit reminder phrasing\",\n",
        "#            ],\n",
        "#        }\n",
        "#     ],\n",
        "#     \"notes\": [\"Mocked classifier output\"],\n",
        "# }\n",
        "#\n",
        "# with canned_llm_response(canned_response):\n",
        "#     classification, slot_result = run_intent_pipeline(\n",
        "#         text=sample_text,\n",
        "#         entities=sample_entities,\n",
        "#         source_type=\"imessage\",\n",
        "#     )\n",
        "#     print(classification)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
